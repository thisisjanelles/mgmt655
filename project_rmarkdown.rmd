---
title: "Predicting Airbnb Prices in New York City"
subtitle: "Data Science Project for MGMT 655 Business Analytics for Decision Making"
author: "Kushal AGARWAL, Janelle Ashley SY, PARK Suel Ki, Sanath Manjunath NADIG"
date: "26 July 2022"
output:
  html_document:
    prettydoc::html_pretty:
      theme: architect
    highlight: espresso
    # css: styles.css
    # latex_engine: xelatex
    # mainfont: Calibri Light
    toc: yes
    toc_float: 
      collapsed: false
      smooth_scroll: false
    number_sections: false
---

```{r setup, include=F}
# Global Setting
knitr::opts_chunk$set(echo = T, 
                      warning = F, 
                      message = F,
                      cache = T,
                      dpi = 600, 
                      fig.width = 10, 
                      fig.height = 6, 
                      fig.align = "center")
```

```{css, echo = F}
h1 { color: rgb(62, 6, 148); }
h2 { color: rgb(0, 104, 139); } 
h3 { color: rgb(51, 122, 183); }

body {font-family:  -apple-system, BlinkMacSystemFont, 
                    "Segoe UI", Roboto, Ubuntu;
      font-size: 12pt; }

code { color: rgb(205,79,57) }

.tocify-extend-page {height: 0 !important; }
```

## 1. Business Question

> As a world's major cultural, financial and commercial hub, New York City attracts travellers from all over the world for personal and business trips. [The Average hotel prices per night in NYC](https://www.kayak.com/New-York-Hotels.15830.hotel.ksp) is USD282. [The average monthly room price in NYC](https://smartasset.com/mortgage/what-is-the-cost-of-living-in-new-york-city)
 ranges from $3,295 to $6,191. NYC is by far [The most expensive in the nation, and nearly three times the national average of $1,463](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7399214/).This fact attracts many potential hosts to Airbnb.

> Airbnb hosts face a major challenge while pricing the room rent for their properties. Renters have a number of filters to such as price, # of bedrooms, room type, and more, but ultimately the amount the host can charge is tied to market price. 

> Airbnb provides help to hosts, but there is no easy access methods to determine the best price to rent out a space. Even the 3rd party apps are very expensive to use. Hosts can use some metric on historical averages but that come with its own issues such as dynamic markets leading to loss of opportunities. 

>Thus, in this project we aim to develop a model to predict the price of Airbnb room that a host can charge in New York City.


## 2. Import

> Load packages

```{r Load Package}
pacman::p_load(tidyverse, lubridate,
               tidymodels,
               skimr, GGally, ggstatsplot, Hmisc, jtools, huxtable, interactions,
               usemodels, ranger, doParallel, vip,
               DT, plotly,
               ggthemes, scales, ggthemr, ggfortify, ggstance, ggalt,
               broom, modelr,
               shiny, shinydashboard,
               finetune, xgboost
)
```

> Data was sourced from [Kaggle](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)

Variable (Feature) Name           | Description
:---------------------------------|:--------------------------------------------------------------------
name                              | Airbnb listing name
host_name                         | Airbnb host name
neighbourhood_group               | "Manhattan", "Brooklyn", "Queens", "Bronx", or "Staten Island"
neighbourhood                     | 
latitude                          | Airbnb latitude coordinates
longitude                         | Airbnb longitude coordinates
room_type                         | "Entire home/apt", "Private room", or "Shared room"
`price`                           | Airbnb listing price per night in US dollars
minimum_nights                    | Minimum number of nights required to book a specific Airbnb listing
number_of_reviews                 | How many reviews the Airbnb listing has received
last_review                       | Date of the latest review on the Airbnb listing
reviews_per_month                 | Number of reviews the Airbnb received/month
calculated_host_listings_count    | Number of listings per host
availability_365                  | Number of days the Airbnb listing is available for booking

```{r Import Data}
input_data <- read_csv("AB_NYC_2019.csv")
```

```{r Data table}
input_data %>% 
  datatable(options = list(scrollX = T))
```  

## 3. Transform & EDA

```{r}
skim(input_data)
```

```{r Transform Data}
cleaned_data <- input_data %>% 
  filter(price != 0) %>%
  select(-name, -host_name, -host_id, -last_review) %>%
  mutate(across(c(neighbourhood_group,
                  room_type,
                  neighbourhood), as.factor)) %>%
  mutate(log10_price = log10(price)) %>%
  select(-price) %>%
  drop_na()
```


```{r Check Transform Data}
skim(cleaned_data)
```

### 3.1 Uni-variate Analysis {.tabset}

> Key demographic variables are analysed to determine the characteristics of Airbnb listings in the sample. 

#### 3.1.1 Log10Price

> 

```{r Price}
ggthemr("fresh")

cleaned_data %>%
  ggplot() +
  geom_density(aes(x = log10_price)) +
  geom_vline(aes(xintercept = mean(log10_price)),
             linetype = "dashed",
             color = "tomato3") +
  annotate(geom = "text",
           color = "tomato3") +
  labs(title = "Log Price per Night of Airbnbs in New York City",
       y = "Density",
       x = "Price in USD")
```

#### 3.1.2 Neighbourhood Group

> The majority of Airbnb listings are located in Brooklyn, Manhattan, and Queens. Minimally occurring values will need to be pooled into an "Other" category.

```{r Neighbourhood Group}
cleaned_data %>%
  ggplot(aes(neighbourhood_group)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = percent) +
  geom_label(aes(label = percent((..count..)/sum(..count..)),
                 y = (..count..)/sum(..count..)), 
             stat = "count",
             size = 5,
             fill = "white") +
  labs(title = "Percentage of Airbnb Listings by Neighborhood Group",
       x = "Neighborhood Group",
       y = "Percentage")
```

#### 3.1.3 Number of Reviews

> 

```{r Number of Reviews}
cleaned_data %>%
  ggplot() +
  geom_density(aes(x = number_of_reviews)) +
  geom_vline(aes(xintercept = mean(number_of_reviews)),
             linetype = "dashed",
             color = "tomato3") +
  annotate(geom = "text",
           color = "tomato3") +
  labs(title = "Number of Reviews on an Airbnb Listing",
       x = "Number of Reviews",
       y = "")
```

<!-- #### 3.1.4 Glucose Level  -->

<!-- > Average glucose level = 106.15. Sample is in the normal range.  -->

<!-- ```{r Glucose Level Distribution} -->
<!-- strokedataset %>% -->
<!--   ggplot() + -->
<!--   geom_density(aes(x = avg_glucose_level)) + -->
<!--   geom_vline(aes(xintercept = mean(avg_glucose_level)), -->
<!--              linetype = "dashed", -->
<!--              color = "tomato3") + -->
<!--   annotate(geom = "text", -->
<!--            label = "Mean = 106.15", -->
<!--            color = "tomato3", -->
<!--            x = 135,  -->
<!--            y = 0.013) + -->
<!--   labs(title = "Avg Glucose Level Distribution of Patients", -->
<!--        y = "Density", -->
<!--        x = "Average Glucose Level") -->

<!-- summary(strokedataset$avg_glucose_level) -->
<!-- ``` -->
<!-- ### {-} -->

### 3.2 Multi-variate Analysis {.tabset}

> The effect size of each quantitative variable is evaluated against stroke to determine variables with the largest effect. As for categorical variables, because data is imbalanced, proportion is used to analyse data between independent variables and stroke. 

> [Hedges' g interpretation](https://www.statisticshowto.com/hedges-g/): Small effect (0.2). Medium Effect (0.5). Large effect (0.8)

#### 3.2.1 Hypertension

> Comparing relative percentage, there is a higher proportion of stroke cases amongst patients who have hypertension (27%) as compared to those without (9%).

```{r Hypertension}
strokedataset %>%
  mutate(stroke = recode(stroke, "1" = "Stroke", "0" = "No Stroke")) %>%
  ggplot(aes(hypertension, group = stroke)) +
  geom_bar(aes
           (y = ..prop.., 
             fill = factor(..x..)), 
           stat = "count", 
           show.legend = FALSE) +
  geom_label(aes(label = percent(..prop..),
                y = ..prop..), 
            stat = "count",
            size = 5,
            fill = "white") +
  labs(title = "Proportion of Patients With Hypertension",
       x = "Hypertension", 
       y = "Percentage", 
       fill = "hypertension") +
  facet_grid(~stroke) +
  scale_y_continuous(labels = percent) +
  scale_x_discrete(labels = c("0" = "No", "1" = "Yes"))
```
#### 3.2.2 Heart Disease

> Similarly, there is a higher proportion of stroke cases amongst patients who have heart disease (19%) as compared to those without (5%).

```{r Heart Disease}
strokedataset %>%
  mutate(stroke = recode(stroke, "1" = "Stroke", "0" = "No Stroke")) %>%
  ggplot(aes(heart_disease, group = stroke)) +
  geom_bar(aes
           (y = ..prop.., 
             fill = factor(..x..)), 
           stat = "count", 
           show.legend = FALSE) +
  geom_label(aes(label = percent(..prop..),
                y = ..prop..), 
            stat = "count",
            size = 5,
            fill = "white") +
  labs(title = "Proportion of Patients With Heart Disease",
       x = "Heart Disease", 
       y = "Percentage", 
       fill = "heart_disease") +
  facet_grid(~stroke) +
  scale_y_continuous(labels = percent) +
  scale_x_discrete(labels = c("0" = "No", "1" = "Yes"))
```

#### 3.2.3 Smoking Status

> There is a higher proportion of stroke cases amongst patients who formerly smoke (28.11%).

```{r Smoke}
strokedataset %>%
  mutate(stroke = recode(stroke, "1" = "Stroke", "0" = "No Stroke")) %>%
  ggplot(aes(smoking_status, group = stroke)) +
  geom_bar(aes
           (y = ..prop.., 
             fill = factor(..x..)), 
           stat = "count", 
           show.legend = FALSE) +
  geom_label(aes(label = percent(..prop..),
                y = ..prop..), 
            stat = "count",
            nudge_y = -0.06,
            size = 5,
            fill = "white") +
   labs(title = "Patient's Smoking Status By Proportion",
       x = "Smoking Status", 
       y = "Percentage", 
       fill = "smoking_status") +
  facet_grid(~stroke) +
  scale_y_continuous(labels = percent) +
  scale_x_discrete(labels = function(x) str_wrap(x , width = 10)) +
  coord_flip()
```

#### 3.2.4 Gender

> There is a relatively similar proportion of patients with stroke across male and females. There is only 1 respondent who identified as "other".

```{r}
strokedataset %>%
  count(gender)

strokedataset_1 <- strokedataset[strokedataset$gender !="Other",]
```

```{r Gender}
strokedataset_1 %>%
  mutate(stroke = recode(stroke, "1" = "Stroke", "0" = "No Stroke")) %>%
  ggplot(aes(gender, group = stroke)) +
  geom_bar(aes
           (y = ..prop.., 
             fill = factor(..x..)), 
           stat = "count", 
           show.legend = FALSE) +
  geom_label(aes(label = percent(..prop..),
                y = ..prop..), 
            stat = "count",
            size = 5,
            fill = "white") +
  labs(title = "Patients with and without stroke by gender",
       x = "Gender", 
       y = "Percentage", 
       fill = "gender") +
  facet_grid(~stroke) +
  scale_y_continuous(labels = percent)
```

#### 3.2.5 BMI & Stroke

> Patients with stroke have higher average BMI, but effect size is small. 

```{r BMI effect size}
strokedataset %>% 
  mutate(stroke = recode(stroke, "1" = "Stroke", "0" = "No Stroke")) %>%
  ggbetweenstats(
    x = stroke,
    y = bmi,
    plot.type = "box")
```

#### 3.2.6 Glucose level

> Effect size is small for females but medium for males. Patients with stroke have higher average glucose level. Male stroke patients saw a higher average glucose level than female stroke patients. 

```{r Avg Glucose Level Effect Size Across Gender}
strokedataset_1 %>%
  mutate(stroke = recode(stroke, "1" = "Stroke", "0" = "No Stroke")) %>%
  grouped_ggbetweenstats(
    x = stroke,
    y = avg_glucose_level,
    grouping.var = gender,
    plot.type = "box")
```

#### 3.2.7 Age & Stroke

> Effect size is large. Patients who have stroke are mostly older than patients who have no stroke. 

```{r Age}
strokedataset %>% 
  mutate(stroke = recode(stroke, "1" = "Stroke", "0" = "No Stroke")) %>%
  ggbetweenstats(
    x = stroke,
    y = age,
    plot.type = "box")
```

### 3.3 Correlation Study {.tabset}

> Prep and bake.

```{r Reciped for EDA}
# RECIPE FOR EDA ----

## Recipe for log10_price ----
recipe_eda <- 
  recipe(formula = log10_price ~ .,
         data = cleaned_data) %>% 
  step_rm(id) %>%
  step_other(neighbourhood, threshold = 0.05) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

## Baking for log10_price ----
baked_eda <- 
  recipe_eda %>% # plan 
  prep() %>% # for calculation
  bake(cleaned_data)

glimpse(baked_eda)
```


> Neighbourhood Group and Room Type appears to have the highest strength in relationship with stroke (>0.1 corr).

#### 3.3.1 Correlation Matrix

```{r Correlation Matrix}
corr_table <- baked_eda %>% 
  as.matrix(.) %>%
  rcorr(.) %>%
  tidy(.) %>%
  rename(var1 = column1,
         var2 = column2,
         CORR = estimate) %>%
  mutate(absCORR = abs(CORR)) %>%
  filter(var1 == "log10_price" | var2 == "log10_price") %>%
  DT::datatable()
```
### {-}

## 4. Predictive Model

> Four predictive models were derived. Model A includes all variables, while Model B focuses on variables with effect sizes and correlation (age, ever_married, avg_glucose_level, heart_disease and hypertension). 

> Model C is derived based on a refinement of both Model A and B based on variable importance, while Model XG seeks to improve on the best model. 

> Because sample data is highly imbalanced (due to natural rare occurrence of stroke), the sample data was tested using step_upsample, step_downsample and step_rose.

### 4.1. Split

> Preparation

```{r}
# SPLITTING ----

set.seed(22062801)

data_split <- 
  cleaned_data %>% 
  initial_split(prop = 0.80)

data_split
```

> Execution

```{r}
## Create Training and Testing Sets ----

data_train <- # training(rent_split)
  data_split %>% 
  training() # 80%

# TESTING ----
# Create Smaller Set to Save on Run Time
data_train <- data_train %>%
  sample_n(5000)

data_test <- 
  data_split %>% 
  testing() # 20%
```

### 4.2. Pre-Process

> Feature Engineering

```{r}
# RECIPE FOR PREDICTION ----
## Recipe 1: Price with BoxCox inputs ----
recipe_price_box <- 
  recipe(formula = `log10_price`~ .,
         data = data_train) %>%
  step_rm(id) %>%
  step_other(neighbourhood, threshold = 0.05) %>%
  step_BoxCox(calculated_host_listings_count) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors())

## Recipe 2: Price without BoxCox inputs ----
recipe_price <- 
  recipe(formula = `log10_price`~ 
           neighbourhood_group + neighbourhood + latitude + longitude +
           room_type + minimum_nights + number_of_reviews + 
           reviews_per_month + availability_365,
         data = data_train) %>% 
  step_other(neighbourhood, threshold = 0.05) %>%
  step_novel(neighbourhood_group, neighbourhood, room_type) %>% 
  step_unknown(neighbourhood_group, neighbourhood, room_type) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors())

# BAKING ----
## Baking for price with BoxCox ----
baked_price_box <- 
  recipe_price_box %>% # plan 
  prep() %>% # for calculation
  bake(data_train)

glimpse(baked_price_box)

## Baking for price without BoxCox ----
baked_price <- 
  recipe_price %>% # plan 
  prep() %>% # for calculation
  bake(data_train) 

glimpse(baked_price)
```

### 4.3. Fit

```{r}
# CREATE MODELS ----
## Random Forest ----

rf_model <- 
  rand_forest() %>%
  set_args(trees = 1000, mtry = tune(), min_n = tune()) %>%
  set_engine("ranger",
             importance = "permutation") %>% 
  set_mode("regression")

## XG Boost ----

XG_BOOST <- # extreme gradient boosting
  boost_tree(trees = 500L,
             mtry = tune(),
             min_n = tune(),
             tree_depth = tune(),
             sample_size = tune(),
             learn_rate = tune()
  ) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

## Linear Regression ----
ols_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")
```

### 4.4. Tune

> Using `workflows::``workflow()`

```{r}
# CREATE WORKFLOWS ----

## Random Forest for price with BoxCox ----
workflow_rf_price_box <-
  workflow() %>% 
  add_recipe(recipe_price_box) %>% 
  add_model(rf_model)

## Random Forest with price ----
workflow_rf_price <-
  workflow() %>% 
  add_recipe(recipe_price) %>% 
  add_model(rf_model)

## XG Boost for price with BoxCox ----
workflow_xg_price_box <- 
  workflow() %>% 
  add_recipe(recipe_price_box) %>% 
  add_model(XG_BOOST)

## XG Boost with price ----
workflow_xg_price <- 
  workflow() %>% 
  add_recipe(recipe_price) %>% 
  add_model(XG_BOOST)

## Linear regression for price with BoxCox ----
workflow_ols_price_box <- 
  workflow() %>% 
  add_recipe(recipe_price_box) %>% 
  add_model(ols_model)

## Linear regression with price ----
workflow_ols_price <- 
  workflow() %>% 
  add_recipe(recipe_price) %>% 
  add_model(ols_model)
```

> Cross-validation (cv)

```{r}
# CROSS VALIDATION ----

## Cross validation for Random Forest ----
set.seed(22062802)

cv_rf <- 
  data_train %>% 
  vfold_cv(v = 10)

cv_rf

## Cross validation for XG BOOST ----

### Score ----
set.seed(22201701)

cv_xg <- 
  data_train %>% 
  vfold_cv(v = 10,
           strata = log10_price) # output variable

cv_xg

## Cross validation for Linear regression ----
set.seed(22062802)

cv_ols <- 
  data_train %>% 
  vfold_cv(v = 10)

cv_ols
```

> Parallel Processing (for Expedited/speedy Modeling)

```{r}
doParallel::registerDoParallel()

# TUNING ----
## Random Forest score tuning ----
### Price with BoxCox ----
tuned_rf_price_box <-
  workflow_rf_price_box %>% 
  tune_grid(resamples = cv_rf,
            grid = 3:10)

### Price ----
tuned_rf_price <-
  workflow_rf_price %>% 
  tune_grid(resamples = cv_rf,
            grid = 3:10)

## XG Boost tuning ----

set.seed(22201702)

grid_xg <-
  grid_max_entropy(
    mtry(c(5L, 10L),
    ),
    min_n(c(10L, 40L)
    ),
    tree_depth(c(5L, 10L)
    ),
    sample_prop(c(0.5, 1.0)
    ),
    learn_rate(c(-2, -1)
    ),
    size = 20
  )

### Price with BoxCox ----
tuned_xg_price_box <- 
  workflow_xg_price_box %>% 
  tune_grid(resamples = cv_xg,
            grid = grid_xg,
            control = control_grid(save_pred = T)
  )

### Price ----
tuned_xg_price <- 
  workflow_xg_price %>% 
  tune_grid(resamples = cv_xg,
            grid = grid_xg,
            control = control_grid(save_pred = T)
  )

## OLS Tuning ----
### Price with BoxCox ----
tuned_ols_price_box <-
  workflow_ols_price_box %>%
  tune_grid(resamples = cv_ols,
            grid = 3:10)

### Price ----
tuned_ols_price <-
  workflow_ols_price %>%
  tune_grid(resamples = cv_ols,
            grid = 3:10)
```


```{r Best Model}
# PARAMETERS AFTER TUNING ----

## Random Forest parameters ----

### Price with box ----
parameters_tuned_rf_price_box <- 
  tuned_rf_price_box %>% 
  select_best(metric = "rmse")

### Price ----
parameters_tuned_rf_price <- 
  tuned_rf_price %>% 
  select_best(metric = "rmse")

## XG Boost parameters ----

### Price with BoxCox ----
parameters_tuned_xg_price_box <-
  tuned_xg_price_box %>%
  select_best(metric = "rmse")

### Price ----
parameters_tuned_xg_price <-
  tuned_xg_price %>%
  select_best(metric = "rmse")

## Linear regression parameters ----

### Price with box ----
parameters_tuned_ols_price_box <- 
  tuned_ols_price_box %>% 
  select_best(metric = "rmse")

### Price ----
parameters_tuned_ols_price <- 
  tuned_ols_price %>% 
  select_best(metric = "rmse")
```


```{r}
# FINALIZE WORKFLOW ----

## Random Forest ----

### Price with box ----
finalized_workflow_rf_price_box <-
  workflow_rf_price_box %>% 
  finalize_workflow(parameters_tuned_rf_price_box)

### Price ----
finalized_workflow_rf_price <-
  workflow_rf_price %>% 
  finalize_workflow(parameters_tuned_rf_price)

## XG Boost ----

### Price with box ----
finalized_workflow_xg_price_box <- 
  workflow_xg_price_box %>% 
  finalize_workflow(parameters_tuned_xg_price_box)

### Price ----
finalized_workflow_xg_price <- 
  workflow_xg_price %>% 
  finalize_workflow(parameters_tuned_xg_price)

## Linear regression ----

### Price with box ----
finalized_workflow_ols_price_box <-
  workflow_ols_price_box %>% 
  finalize_workflow(parameters_tuned_ols_price_box)

### Price ----
finalized_workflow_ols_price <-
  workflow_ols_price %>% 
  finalize_workflow(parameters_tuned_ols_price)
```

### 4.5. Assess

```{r}
# LAST FIT ----

## Random Forest ----

### Price with box ----
fit_rf_price_box <-
  finalized_workflow_rf_price_box %>% 
  last_fit(data_split)

### Price ----
fit_rf_price <-
  finalized_workflow_rf_price %>% 
  last_fit(data_split)

## XG Boost ----

### Price with box ----
fit_xg_price_box <-
  finalized_workflow_xg_price_box %>% 
  last_fit(data_split)

### Price ----
fit_xg_price <-
  finalized_workflow_xg_price %>% 
  last_fit(data_split)

## Linear regression ----

### Price with box ----
fit_ols_price_box <-
  finalized_workflow_ols_price_box %>% 
  last_fit(data_split)

### Price ----
fit_ols_price <-
  finalized_workflow_ols_price %>% 
  last_fit(data_split)
```

> Model Performance 

```{r Comparing 6 models}
# PERFORMANCE ----

## Random Forest ----

### Price with BoxCox ----
performance_rf_price_box <- 
  fit_rf_price_box %>% 
  collect_metrics() %>% 
  mutate(algorithm = "Random Forest for price with BoxCox")

### Price ----
performance_rf_price <- 
  fit_rf_price %>% 
  collect_metrics() %>% 
  mutate(algorithm = "Random Forest for price")

## XG Boost ----

### Price with BoxCox ----
performance_xg_price_box <- 
  fit_xg_price_box %>% # for_performance(fit_xg) %>% 
  collect_metrics() %>%
  mutate(algorithm = "XG Boost for price with BoxCox")

### Price ----
performance_xg_price <- 
  fit_xg_price %>% # for_performance(fit_xg) %>% 
  collect_metrics() %>%
  mutate(algorithm = "XG Boost for price")

## Linear regression ----

### Price with BoxCox ----
performance_ols_price_box <- 
  fit_ols_price_box %>% 
  collect_metrics() %>% 
  mutate(algorithm = "Linear regression for price with BoxCox")

### Price ----
performance_ols_price <- 
  fit_ols_price %>% 
  collect_metrics() %>% 
  mutate(algorithm = "Linear regression for price")

# COMPARE PERFORMANCE OF DIFFERENT ALGORITHMS AND RECIPES ----

bind_rows(performance_rf_price_box,
          performance_rf_price,
          performance_xg_price_box,
          performance_xg_price,
          performance_ols_price_box, 
          performance_ols_price) %>%
  select(-.estimator,
         -.config) %>% 
  pivot_wider(names_from = .metric,
              values_from = .estimate) %>% 
  datatable() %>% 
  formatRound(columns = c("rmse",
                          "rsq"),
              digits = 2)
```

> Collect predictions

```{r}
# CHECK PREDICTIONS ----
## Random Forest ----
### Price with BoxCox ----
prediction_rf_price_box <- 
  fit_rf_price_box %>% 
  collect_predictions() %>%
  mutate(algorithm = "Random Forest for price with BoxCox")

### Price ----
prediction_rf_price <- 
  fit_rf_price %>% 
  collect_predictions() %>%
  mutate(algorithm = "Random Forest for price")

## XG Boost ----
### Price with BoxCox ----
prediction_xg_price_box <-
  fit_xg_price_box %>%
  collect_predictions() %>%
  mutate(algorithm = "XG Boost for price with BoxCox") 

### Price ----
prediction_xg_price <-
  fit_xg_price %>%
  collect_predictions() %>%
  mutate(algorithm = "XG Boost for price")

## Linear regression ----
### Price with BoxCox ----
prediction_ols_price_box <- 
  fit_ols_price_box %>% 
  collect_predictions() %>%
  mutate(algorithm = "Linear regression for price with BoxCox")

### Price ----
prediction_ols_price <- 
  fit_ols_price %>% 
  collect_predictions() %>%
  mutate(algorithm = "Linear regression for price")

# CONSOLIDATE PREDICTIONS ----

cleaned_data <- 
  cleaned_data %>% 
  tibble::rowid_to_column(".row")

data_test_with_row <- 
  data_test %>%
  rename(.row = id)
```

> Confusion Matrix

```{r}
CM_builder_for_MGMT655 <- 
  function(data, outcome)
{ 
  {data} %>% 
    conf_mat({outcome}, .pred_class) %>% 
    pluck(1) %>% 
    as_tibble() %>% 
    mutate(cm_colors = ifelse(Truth == "1" & Prediction == "1", "True Positive",
                              ifelse(Truth == "1" & Prediction == "0", "False Negative",
                                     ifelse(Truth == "0" & Prediction == "1", 
                                            "False Positive", 
                                            "True Negative")
                              )
    )
    ) %>% 
    ggplot(aes(x = Prediction, y = Truth)) + 
    geom_tile(aes(fill = cm_colors), show.legend = F) +
    scale_fill_manual(values = c("True Positive" = "green",
                                 "False Negative" = "red",
                                 "False Positive" = "red",
                                 "True Negative" = "green")
    ) + 
    geom_text(aes(label = n), color = "white", size = 10) + 
    geom_label(aes(label = cm_colors), vjust = 2
    ) + 
    theme_fivethirtyeight() + 
    theme(axis.title = element_text()
    ) 
}
```

```{r}
CM_builder_for_MGMT655(prediction_XG, "stroke")
```

> Null Model

```{r}
model_baseline <- null_model() %>% 
  set_engine("parsnip") %>% 
  set_mode("classification")

workflow_baseline <- workflow() %>% 
  add_recipe(reciped_1) %>% 
  add_model(model_baseline) %>% 
  fit_resamples(cv10,
                control = control_resamples(save_pred = T))

performance_baseline <- workflow_baseline %>% collect_metrics()
prediction_baseline <- workflow_baseline %>% collect_predictions()
```

> Model Plots

```{r}
# CREATE PLOTS ----
## Random Forest ----

### Price with BoxCox ----
rf_price_box_plot <- 
  prediction_rf_price_box %>% 
  select(.row, log10_price, .pred)%>%
  ggplot(aes(x = log10_price,
             y = .pred)
  ) + 
  geom_point(color = "dodgerblue",
             alpha = 0.50) + 
  geom_abline(color = "red",
              lty = 2) +
  labs(y = "Predicted Price of Airbnb",
       x = "Actual price of Airbnb",
       title = "Predicting Airbnb price in NYC using RF with BoxCox") + 
  theme_bw()

### Price ----
rf_price_plot <- 
  prediction_rf_price %>% 
  select(.row, log10_price, .pred) %>%
  ggplot(aes(x = log10_price,
             y = .pred)
  ) + 
  geom_point(color = "dodgerblue",
             alpha = 0.50) + 
  geom_abline(color = "red",
              lty = 2) +
  labs(y = "Predicted Price of Airbnb",
       x = "Actual price of Airbnb",
       title = "Predicting Airbnb price in NYC using RF") + 
  theme_bw()

## XG Boost ----

###Price with BoxCox ----

xg_price_box_plot <- 
  prediction_xg_price_box %>% 
  select(.row, log10_price, .pred) %>%
  ggplot(aes(x = log10_price,
             y = .pred)
  ) + 
  geom_point(color = "dodgerblue",
             alpha = 0.50) + 
  geom_abline(color = "red",
              lty = 2) +
  labs(y = "Predicted Price of Airbnb",
       x = "Actual price of Airbnb",
       title = "Predicting Airbnb price in NYC using XG with BoxCox") + 
  theme_bw()

### Price ----
xg_price_plot <- 
  prediction_rf_price %>% 
  select(.row, log10_price, .pred) %>%
  ggplot(aes(x = log10_price,
             y = .pred)
  ) + 
  geom_point(color = "dodgerblue",
             alpha = 0.50) + 
  geom_abline(color = "red",
              lty = 2) +
  labs(y = "Predicted Price of Airbnb",
       x = "Actual price of Airbnb",
       title = "Predicting Airbnb price in NYC using XG") + 
  theme_bw()

## OLS ----

### Price with BoxCox ----

ols_price_box_plot <- 
  prediction_ols_price_box %>% 
  select(.row, log10_price, .pred) %>%
  ggplot(aes(x = log10_price,
             y = .pred)
  ) + 
  geom_point(color = "dodgerblue",
             alpha = 0.50) + 
  geom_abline(color = "red",
              lty = 2) +
  labs(y = "Predicted Price of Airbnb",
       x = "Actual price of Airbnb",
       title = "Predicting Airbnb price in NYC using OLS with BoxCox") + 
  theme_bw()

ols_price_plot <- 
  prediction_ols_price %>% 
  select(.row, log10_price, .pred) %>%
  ggplot(aes(x = log10_price,
             y = .pred)
  ) + 
  geom_point(color = "dodgerblue",
             alpha = 0.50) + 
  geom_abline(color = "red",
              lty = 2) +
  labs(y = "Predicted Price of Airbnb",
       x = "Actual price of Airbnb",
       title = "Predicting Airbnb price in NYC using OLS") + 
  theme_bw()

grid.arrange(rf_price_box_plot,
             rf_price_plot,
             xg_price_box_plot,
             xg_price_plot,
             ols_price_box_plot,
             ols_price_plot,
             ncol = 2)
```

> Feature Importance

```{r}
# FINALIZED MODEL ----
## Feature Importance ----

library(vip)

# VERY IMPORTANT ----
finalized_model <- 
  finalized_workflow_xg_price %>% 
  fit(cleaned_data)

feature_importance_PREP <-
  XG_BOOST %>% # model
  finalize_model(select_best(tuned_xg_price)
  ) %>%
  set_engine("xgboost",
             importance = "permutation")

feature_importance <- 
  workflow() %>% 
  add_recipe(recipe_price) %>% 
  add_model(feature_importance_PREP) %>% 
  fit(data_train) %>% 
  extract_fit_parsnip() %>% # pull_workflow_fit()
  vip(aesthetic = list(fill = "deepskyblue2",
                       alpha = 0.50)
  ) 

feature_importance + 
  theme_bw()
```

> Deploy Machine Learning Algorithm to Dashboard

```{r}
finalized_model %>% saveRDS("finalized_model.rds")
```

> Save R Data 

```{r}
save.image("project_airbnb.RData")
```

## 5. Executive Summary

### 5.1. Evidence

* We tested 6 models for this predictive modeling task.

* Our first model`Random Forest ALgorithm A` up-sampled the minority class (non-stroke) and included all variables, excluding ID. 

* Based on our correlation analysis of all variables, **age**, **heart disease**, **average glucose level**, **hypertension** and **marriage** had the strongest impact on stroke. 

* Hence, the second model `Random Forest Algorithm B` focused on these 5 variables. An interaction between average glucose level, hypertension and heart disease was also factored into the recipe. 

* Based on variable importance graph, the variables "smoking_status" and "bmi" was included into `Random Forest Algorithm C` in an attempt to improve overall performance. 

* Based on `Random Forest Algorithm B`, we attempted to use the same variables with the computation method of XG Boost to improve the model performance.

* Of the 4 models, the fourth model `XG Boost Algorithm` performed better with higher roc_auc of **0.86** and higher accuracy of **0.90**, compared to the other three models.

* The top three variables that had the highest importance on the prediction results are **age**, followed by **hypertension** and **heart disease**.

### 5.2. Interpretation

* `XG Boost Algorithm` performed better overall with a roc_auc of **0.86** and accuracy of **0.90**, giving a 10% chance of error. 

* The model focused on the top 5 variables that have the largest effect on stroke based on the feature importance graph, and the relationship between average glucose level to hypertension and heart disease. 

* This implies the significance of focusing on the most important features during the predictive modelling task, and also research on interaction between variables to achieve optimal performance.

* In terms of variable importance, the top 3 variables are age, heart disease and hypertension. The effect size of these variables means `higher level of age, exisiting conditions of hypertension or heart disease` increases the chances of stroke detected. 

### 5.3. Recommendations

* The key use application for this predictive model is in determining whether a person is likely to be diagnosed with stroke based on their lifestyle and health measurements. Hence, the key stakeholders whom would most benefit from this predictive model are `(1) general practitioners`, `(2) patients and family members` and `(3) the public`. 

* This algorithm takes into account the 5 most important variables to produce a evidence-based prediction of the likelihood that a patient has stroke. For `general practitioners`, this predictive model is a useful data tool that can be used in conjunction with qualitative checks conducted based on their expertise. This would help them make **more accurate diagnosis** and recommend patients for further test (i.e. CT scan) and treatments. 

* In addition, this predictive model to **stratify patients into risk groups**. Without this tool, patients would belong to either stroke-positive or stroke-negative groups. However, with this predictive model that produces **quantitative likelihood of getting stroke**, general practitioners can derive a better understanding of the risk levels of a patient getting stroke (e.g. >50%) and thereby conduct necessary checks and provide pertinent advice on lifestyle plans to mitigate the onset of stroke.

* One of the drivers for this predictive model study was the [sharp rise in stroke cases in Singapore](https://www.straitstimes.com/singapore/8300-stroke-cases-admitted-to-public-hospitals-in-2018-public-outreach-campaign-starts) leading to health implications, which could have been avoided through early detection, so that lifestyle modifications and treatment can be provided.

* Hence, it is recommended that this prediction model be **made openly available to public to access and input their lifestyle factors and health measurements** to arrive at a predicted percentage likelihood of being stroke-positive. Singaporeans with higher likelihood of stroke (due to family history of stroke risk factors or lifestyle conditions) could also use this model to understand the risks so that they can make informed decisions on their next steps.

## Limitations

* This study applies to Airbnb prices in NYC only. 

* The results derived from this predictive model only indicates specific neighbor groups. It does not provide information about areas outside available neighbor groups in the data set. 

* This model is easy and accessible to use by giving general idea of prices set by the owners. For outlier room types, this model would not fit.  

* However, the data set lacks other potential outside factors such as travel disruptions caused by COVID 19. Hence, further data collection and analysis if needed must be pursued before this model can be applied to test appropriate price range. 


## References

* [Kaggle Data](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)

* [Average hotel prices per night in NYC](https://www.kayak.com/New-York-Hotels.15830.hotel.ksp)

* [The average monthly room price in NYC](https://smartasset.com/mortgage/what-is-the-cost-of-living-in-new-york-city)

* [The most expensive room price compared by the national average](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7399214/)

## Appendix

* Nil.

## Contribution Statement

> The `pRoject` team worked together to deliver the assignment. Together we defined the problem, identifed the data source and segregated tasks to deliver the final output.

   ***R-script** was led by `Sanath Manjunath Nadig`

  ***Shiny-dashboard** was led by `Janelle Ashley SY`

  ***R-markdown** was led by `Kushal Agarwal, Suel Ki Park`

> `Sanath Manjunath Nadig's` contributions:

1)	Collaborated with team to develop the R-script – Transform & EDA, Split, Pre-process, Fit, Tune and Assess.

2) Collaborated to test and find the best recipe and model with rmse.

3) Conducted research to discover interaction between variables.
  
4) Analyzed data to identify top variables that impact market price of Airbnb in NYC.
  
>  `Kushal Agarwal's` contributions:

1) Collaborated with team to develop the R-script – Transform & EDA, Split, Pre-process, Fit, Tune and Assess.

2) Collaborated with teammate to formulate the business problem.
  
3)  Partnered with team to draft the Rmarkdown file.

4)  Helped in writing the Executive Summary – Evidence, Interpretation and Recommendations.

>  `Janelle Ashley SY` contributions:

1) Collaborated with team to develop the R-script – Transform & EDA, Split, Pre-process, Fit, Tune and Assess.

2)  Partnered with team for successful deployment of R-dashboard .
  
3)  Updated the dashboard according to our results to create the Pricing Airbnb Prediction App.
 
4)	Finalized the models, tested model on Dashboard, added information to Rmarkdown file. 

>  `Suel Ki Park's` contributions:

1) Collaborated with team to develop the R-script – Transform & EDA, Split, Pre-process, Fit, Tune and Assess.

2)  Collaborated with teammate to formulate the business problem.
 
3)  Partnered with team to draft the Rmarkdown file.

4)  Helped in writing the Executive Summary – Evidence, Interpretation and Recommendations.

<br>