---
title: "Predicting Airbnb Prices in New York City"
subtitle: "Data Science Project for MGMT 655 Business Analytics for Decision Making"
author: "Sanath Manjunath NADIG, Janelle Ashley SY, Kushal AGARWAL, PARK Suel Ki"
date: "26 July 2022"
output:
  html_document:
    prettydoc::html_pretty:
      theme: architect
    highlight: espresso
    # css: styles.css
    # latex_engine: xelatex
    # mainfont: Calibri Light
    toc: yes
    toc_float: 
      collapsed: false
      smooth_scroll: false
    number_sections: false
---

```{r setup, include=F}
# Global Setting
knitr::opts_chunk$set(echo = T, 
                      warning = F, 
                      message = F,
                      cache = T,
                      dpi = 600, 
                      fig.width = 10, 
                      fig.height = 6, 
                      fig.align = "center")
```

```{css, echo = F}
h1 { color: rgb(62, 6, 148); }
h2 { color: rgb(0, 104, 139); } 
h3 { color: rgb(51, 122, 183); }

body {font-family:  -apple-system, BlinkMacSystemFont, 
                    "Segoe UI", Roboto, Ubuntu;
      font-size: 12pt; }

code { color: rgb(205,79,57) }

.tocify-extend-page {height: 0 !important; }
```

## 1. Business Question

> As a world's major cultural, financial and commercial hub, New York City attracts travellers from all over the world for personal and business trips. [The Average hotel prices per night in NYC](https://www.kayak.com/New-York-Hotels.15830.hotel.ksp) is USD282. [The average monthly room price in NYC](https://smartasset.com/mortgage/what-is-the-cost-of-living-in-new-york-city)
 ranges from $3,295 to $6,191. NYC is by far [The most expensive in the nation, and nearly three times the national average of $1,463](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7399214/).This fact attracts many potential hosts to Airbnb.

> Airbnb hosts face a major challenge while pricing the room rent for their properties. Renters have a number of filters to such as price, # of bedrooms, room type, and more, but ultimately the amount the host can charge is tied to market price. 

> Airbnb provides help to hosts, but there is no easy access methods to determine the best price to rent out a space. Even the 3rd party apps are very expensive to use. Hosts can use some metric on historical averages but that come with its own issues such as dynamic markets leading to loss of opportunities. 

> Thus, in this project we aim to develop a model to predict the price of Airbnb room that a host can charge in New York City.

## 2. Import

> Load packages

```{r Load Package}
pacman::p_load(tidyverse, lubridate,
               tidymodels,
               skimr, GGally, ggstatsplot, Hmisc, jtools, huxtable, interactions,
               usemodels, ranger, doParallel, vip,
               DT, plotly,
               ggthemes, scales, ggthemr, ggfortify, ggstance, ggalt,
               broom, modelr,
               shiny, shinydashboard,
               finetune, xgboost
)
```

> Data was sourced from [Kaggle](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)

Variable (Feature) Name           | Description
:---------------------------------|:--------------------------------------------------------------------
name                              | Airbnb listing name
host_name                         | Airbnb host name
neighbourhood_group               | "Manhattan", "Brooklyn", "Queens", "Bronx", or "Staten Island"
neighbourhood                     | Neighbourhood of the Airbnb (out of 218 possible neighbourhoods)
latitude                          | Airbnb latitude coordinates
longitude                         | Airbnb longitude coordinates
room_type                         | "Entire home/apt", "Private room", or "Shared room"
`price`                           | Airbnb listing price per night in US dollars
minimum_nights                    | Minimum number of nights required to book a specific Airbnb listing
number_of_reviews                 | How many reviews the Airbnb listing has received
last_review                       | Date of the latest review on the Airbnb listing
reviews_per_month                 | Number of reviews the Airbnb received/month
calculated_host_listings_count    | Number of listings per host
availability_365                  | Number of days the Airbnb listing is available for booking

```{r Import Data}
input_data <- read_csv("AB_NYC_2019.csv")
```

```{r Data table}
input_data %>% 
  datatable(options = list(scrollX = T))
```  

## 3. Transform & EDA

```{r}
skim(input_data)
```

```{r Transform Data}
cleaned_data <- input_data %>% 
  filter(price != 0) %>%
  select(-name, -host_name, -host_id, -last_review) %>%
  mutate(across(c(neighbourhood_group,
                  room_type,
                  neighbourhood), as.factor)) %>%
  mutate(log10_price = log10(price)) %>%
  select(-price) %>%
  drop_na()
```


```{r Check Transform Data}
skim(cleaned_data)
```

### 3.1 Variable Analysis {.tabset}

> Key demographic variables are analysed to determine the characteristics of Airbnb listings in the sample. 

#### 3.1.1 Neighbourhood Group

> The majority of Airbnb listings are located in Brooklyn, Manhattan, and Queens. Minimally occurring values will need to be pooled into an "Other" category.

```{r Neighbourhood Group}
ggthemr("fresh")

cleaned_data %>%
  ggplot(aes(neighbourhood_group)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = percent) +
  geom_label(aes(label = percent((..count..)/sum(..count..)),
                 y = (..count..)/sum(..count..)), 
             stat = "count",
             size = 5,
             fill = "white") +
  labs(title = "Percentage of Airbnb Listings by Neighborhood Group",
       x = "Neighborhood Group",
       y = "Percentage")
```

#### 3.1.2 Average Price VS Neighbourhood Group

```{r Average Price VS Neighbourhood Group}
cleaned_data %>%
  group_by(neighbourhood_group) %>%
  summarise(avg_price = mean(log10_price)) %>%
  mutate(avg_price = 10^avg_price) %>%
  ggplot(aes(x = neighbourhood_group, y = avg_price)) +
  geom_bar(stat = "identity", show.legend = F) +
  scale_y_continuous(labels = dollar) +
  geom_label(aes(label = round(avg_price, digits = 2),
                 y = avg_price, 
                 stat = "count",
                 size = 5,
                 fill = "white")) +
  labs(title = "Average price of Airbnb Listings by Neighborhood Group",
       x = "Neighborhood Group",
       y = "Average Price")
theme_bw
```

#### 3.1.3 Average Price VS Room Type

```{r Average Price VS Room Type}
cleaned_data %>%
  group_by(room_type) %>%
  summarise(avg_price = mean(log10_price)) %>%
  mutate(avg_price = 10^avg_price) %>%
  ggplot(aes(x = room_type, y = avg_price)) +
  geom_bar(stat = "identity", show.legend = F) +
  scale_y_continuous(labels = dollar) +
  geom_label(aes(label = round(avg_price, digits = 2),
                 y = avg_price, 
                 stat = "count")) +
  labs(title = "Average price of Airbnb Listings by Room type",
       x = "Room Type",
       y = "Average Price")
theme_bw()
```

#### 3.1.4 Longitude VS log10Price

```{r Longitude VS log10Price}
cleaned_data %>%
  ggplot(aes(x = longitude, y = log10_price)) + 
  geom_point(color = "dodgerblue",
             alpha = 0.3) +
  geom_smooth(method = "loess",
              formula = y ~ x,
              se = F,
              color = "purple") +
  geom_smooth(method = "lm",
              formula = y ~ x,
              se = F,
              color = "green") +
  geom_smooth(method = "lm",
              formula = y ~ poly(x, degree = 2),
              color = "tomato3") +
  theme_bw()
```

#### 3.1.5 Latitude VS log10Price

```{r Latitude VS log10Price}
cleaned_data %>%
  ggplot(aes(x = latitude, y = log10_price)) + 
  geom_point(color = "dodgerblue",
             alpha = 0.3) +
  geom_smooth(method = "loess",
              formula = y ~ x,
              se = F,
              color = "purple") +
  geom_smooth(method = "lm",
              formula = y ~ x,
              se = F,
              color = "green") +
  geom_smooth(method = "lm",
              formula = y ~ poly(x, degree = 2),
              color = "tomato3") +
  theme_bw()
```

#### 3.1.6 Number of Reviews VS log10 Price

```{r Number of Reviews VS log10 Price}
cleaned_data %>%
  ggplot(aes(x = number_of_reviews, y = log10_price)) + 
  geom_point(color = "dodgerblue",
             alpha = 0.3) +
  geom_smooth(method = "loess",
              formula = y ~ x,
              se = F,
              color = "purple") +
  geom_smooth(method = "lm",
              formula = y ~ x,
              se = F,
              color = "green") +
  geom_smooth(method = "lm",
              formula = y ~ poly(x, degree = 2),
              color = "tomato3") +
  theme_bw()
```

#### 3.1.7 Host Listings Count VS log10Price

```{r Host Listings Count VS log10Price}
cleaned_data %>%
  ggplot(aes(x = calculated_host_listings_count, y = log10_price)) + 
  geom_point(color = "dodgerblue",
             alpha = 0.3) +
  geom_smooth(method = "loess",
              formula = y ~ x,
              se = F,
              color = "purple") +
  geom_smooth(method = "lm",
              formula = y ~ x,
              se = F,
              color = "green") +
  geom_smooth(method = "lm",
              formula = y ~ poly(x, degree = 2),
              color = "tomato3") +
  theme_bw()
```

#### 3.1.8 Availability_365 VS log10Price

```{r Availability_365 VS log10Price}
cleaned_data %>%
  ggplot(aes(x = availability_365, y = log10_price)) + 
  geom_point(color = "dodgerblue",
             alpha = 0.3) +
  geom_smooth(method = "loess",
              formula = y ~ x,
              se = F,
              color = "purple") +
  geom_smooth(method = "lm",
              formula = y ~ x,
              se = F,
              color = "green") +
  geom_smooth(method = "lm",
              formula = y ~ poly(x, degree = 2),
              color = "tomato3") +
  theme_bw()
```
### {-}

### 3.2 Correlation Study {.tabset}

> Prep and bake.

```{r Reciped for EDA}
# RECIPE FOR EDA ----

## Recipe for log10_price ----
recipe_eda <- 
  recipe(formula = log10_price ~ .,
         data = cleaned_data) %>% 
  step_rm(id) %>%
  step_other(neighbourhood, threshold = 0.05) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

## Baking for log10_price ----
baked_eda <- 
  recipe_eda %>% # plan 
  prep() %>% # for calculation
  bake(cleaned_data)

glimpse(baked_eda)
```


> Neighbourhood Group and Room Type appears to have the highest strength in relationship with stroke (>0.1 corr).

#### 3.2.1 Correlation Matrix

```{r Correlation Matrix}
# CORRELATION ----

## Correlation score ----

corr_table <- baked_eda %>% 
  as.matrix(.) %>%
  rcorr(.) %>%
  tidy(.) %>%
  rename(var1 = column1,
         var2 = column2,
         CORR = estimate) %>%
  mutate(absCORR = abs(CORR)) %>%
  filter(var1 == "log10_price" | var2 == "log10_price") %>%
  DT::datatable()

corr_table
```
### {-}

## 4. Predictive Model

> Three predictive models were derived. The modes of each of these models was set to Regression.

> Model 1 is based on Random Forest, Model 2 is based on XG Boost while Model 3 is a simple Original Least Squares (OLS) regression model.

### 4.1. Split

> Preparation

```{r Splitting}
# SPLITTING ----

set.seed(22062801)

data_split <- 
  cleaned_data %>% 
  initial_split(prop = 0.80)

data_split
```

> Execution

```{r Training and Testing Sets}
## Create Training and Testing Sets ----

data_train <- # training(rent_split)
  data_split %>% 
  training() # 80%

data_test <- 
  data_split %>% 
  testing() # 20%
```

### 4.2. Pre-Process: Feature Engineering

> Based on the observations of feature engineering, two recipes were created. In both recipes, the log10_price variable was used as the output variable. Since the dataset contained a lot of neighbourhoods, all neighbourhoods that represented less than 5% of total data were classified as 'Other' neighbourhoods.

> In recipe 1, log10_price was expressed as a function of all other variables remaining after cleaning the data. The variable 'calculated_host_listings_count' was subjected to a BoxCox transformation using the step_BoxCox function.

> In recipe 2, log10_price was expressed as a function of certain variables chosen depending on their importance observed through the correlation matrix. This recipe did not contain the variable that was subjected to BoxCox transformation.

```{r Feature Engineering}
# RECIPE FOR PREDICTION ----
## Recipe 1: Price with BoxCox inputs ----
recipe_price_box <- 
  recipe(formula = `log10_price`~ .,
         data = data_train) %>%
  step_rm(id) %>%
  step_other(neighbourhood, threshold = 0.05) %>%
  step_BoxCox(calculated_host_listings_count) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors())

## Recipe 2: Price without BoxCox inputs ----
recipe_price <- 
  recipe(formula = `log10_price`~ 
           neighbourhood_group + neighbourhood + latitude + longitude +
           room_type + minimum_nights + number_of_reviews + 
           reviews_per_month + availability_365,
         data = data_train) %>% 
  step_other(neighbourhood, threshold = 0.05) %>%
  step_novel(neighbourhood_group, neighbourhood, room_type) %>% 
  step_unknown(neighbourhood_group, neighbourhood, room_type) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors())

# BAKING ----
## Baking for price with BoxCox ----
baked_price_box <- 
  recipe_price_box %>% # plan 
  prep() %>% # for calculation
  bake(data_train)

glimpse(baked_price_box)

## Baking for price without BoxCox ----
baked_price <- 
  recipe_price %>% # plan 
  prep() %>% # for calculation
  bake(data_train) 

glimpse(baked_price)
```

### 4.3. Fit

```{r Fit}
# CREATE MODELS ----
## Random Forest ----

rf_model <- 
  rand_forest() %>%
  set_args(trees = 1000, mtry = tune(), min_n = tune()) %>%
  set_engine("ranger",
             importance = "permutation") %>% 
  set_mode("regression")

## XG Boost ----

XG_BOOST <- # extreme gradient boosting
  boost_tree(trees = 500L,
             mtry = tune(),
             min_n = tune(),
             tree_depth = tune(),
             sample_size = tune(),
             learn_rate = tune()
  ) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

## Linear Regression ----
ols_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")
```

### 4.4. Tune

> Using `workflows::``workflow()`

```{r Tune}
# CREATE WORKFLOWS ----

## Random Forest for price with BoxCox ----
workflow_rf_price_box <-
  workflow() %>% 
  add_recipe(recipe_price_box) %>% 
  add_model(rf_model)

## Random Forest with price ----
workflow_rf_price <-
  workflow() %>% 
  add_recipe(recipe_price) %>% 
  add_model(rf_model)

## XG Boost for price with BoxCox ----
workflow_xg_price_box <- 
  workflow() %>% 
  add_recipe(recipe_price_box) %>% 
  add_model(XG_BOOST)

## XG Boost with price ----
workflow_xg_price <- 
  workflow() %>% 
  add_recipe(recipe_price) %>% 
  add_model(XG_BOOST)

## Linear regression for price with BoxCox ----
workflow_ols_price_box <- 
  workflow() %>% 
  add_recipe(recipe_price_box) %>% 
  add_model(ols_model)

## Linear regression with price ----
workflow_ols_price <- 
  workflow() %>% 
  add_recipe(recipe_price) %>% 
  add_model(ols_model)
```

> Cross-validation (cv)

```{r Cross Validation}
# CROSS VALIDATION ----

## Cross validation for Random Forest ----
set.seed(22062802)

cv_rf <- 
  data_train %>% 
  vfold_cv(v = 10)

## Cross validation for XG BOOST ----

### Score ----
set.seed(22201701)

cv_xg <- 
  data_train %>% 
  vfold_cv(v = 10,
           strata = log10_price) # output variable

## Cross validation for Linear regression ----
set.seed(22062802)

cv_ols <- 
  data_train %>% 
  vfold_cv(v = 10)
```

> Parallel Processing (for expedited/speedy modeling)

```{r Parallel Processing}
doParallel::registerDoParallel()

# TUNING ----
## Random Forest score tuning ----
### Price with BoxCox ----
tuned_rf_price_box <-
  workflow_rf_price_box %>% 
  tune_grid(resamples = cv_rf,
            grid = 3:10)

### Price ----
tuned_rf_price <-
  workflow_rf_price %>% 
  tune_grid(resamples = cv_rf,
            grid = 3:10)

## XG Boost tuning ----

set.seed(22201702)

grid_xg <-
  grid_max_entropy(
    mtry(c(5L, 10L),
    ),
    min_n(c(10L, 40L)
    ),
    tree_depth(c(5L, 10L)
    ),
    sample_prop(c(0.5, 1.0)
    ),
    learn_rate(c(-2, -1)
    ),
    size = 20
  )

### Price with BoxCox ----
tuned_xg_price_box <- 
  workflow_xg_price_box %>% 
  tune_grid(resamples = cv_xg,
            grid = grid_xg,
            control = control_grid(save_pred = T)
  )

### Price ----
tuned_xg_price <- 
  workflow_xg_price %>% 
  tune_grid(resamples = cv_xg,
            grid = grid_xg,
            control = control_grid(save_pred = T)
  )

## OLS Tuning ----
### Price with BoxCox ----
tuned_ols_price_box <-
  workflow_ols_price_box %>%
  tune_grid(resamples = cv_ols,
            grid = 3:10)

### Price ----
tuned_ols_price <-
  workflow_ols_price %>%
  tune_grid(resamples = cv_ols,
            grid = 3:10)
```


```{r Best Model}
# PARAMETERS AFTER TUNING ----

## Random Forest parameters ----

### Price with box ----
parameters_tuned_rf_price_box <- 
  tuned_rf_price_box %>% 
  select_best(metric = "rmse")

### Price ----
parameters_tuned_rf_price <- 
  tuned_rf_price %>% 
  select_best(metric = "rmse")

## XG Boost parameters ----

### Price with BoxCox ----
parameters_tuned_xg_price_box <-
  tuned_xg_price_box %>%
  select_best(metric = "rmse")

### Price ----
parameters_tuned_xg_price <-
  tuned_xg_price %>%
  select_best(metric = "rmse")

## Linear regression parameters ----

### Price with box ----
parameters_tuned_ols_price_box <- 
  tuned_ols_price_box %>% 
  select_best(metric = "rmse")

### Price ----
parameters_tuned_ols_price <- 
  tuned_ols_price %>% 
  select_best(metric = "rmse")
```


```{r Finalize Workflow}
# FINALIZE WORKFLOW ----

## Random Forest ----

### Price with box ----
finalized_workflow_rf_price_box <-
  workflow_rf_price_box %>% 
  finalize_workflow(parameters_tuned_rf_price_box)

### Price ----
finalized_workflow_rf_price <-
  workflow_rf_price %>% 
  finalize_workflow(parameters_tuned_rf_price)

## XG Boost ----

### Price with box ----
finalized_workflow_xg_price_box <- 
  workflow_xg_price_box %>% 
  finalize_workflow(parameters_tuned_xg_price_box)

### Price ----
finalized_workflow_xg_price <- 
  workflow_xg_price %>% 
  finalize_workflow(parameters_tuned_xg_price)

## Linear regression ----

### Price with box ----
finalized_workflow_ols_price_box <-
  workflow_ols_price_box %>% 
  finalize_workflow(parameters_tuned_ols_price_box)

### Price ----
finalized_workflow_ols_price <-
  workflow_ols_price %>% 
  finalize_workflow(parameters_tuned_ols_price)
```

### 4.5. Assess

```{r Assess}
# LAST FIT ----

## Random Forest ----

### Price with box ----
fit_rf_price_box <-
  finalized_workflow_rf_price_box %>% 
  last_fit(data_split)

### Price ----
fit_rf_price <-
  finalized_workflow_rf_price %>% 
  last_fit(data_split)

## XG Boost ----

### Price with box ----
fit_xg_price_box <-
  finalized_workflow_xg_price_box %>% 
  last_fit(data_split)

### Price ----
fit_xg_price <-
  finalized_workflow_xg_price %>% 
  last_fit(data_split)

## Linear regression ----

### Price with box ----
fit_ols_price_box <-
  finalized_workflow_ols_price_box %>% 
  last_fit(data_split)

### Price ----
fit_ols_price <-
  finalized_workflow_ols_price %>% 
  last_fit(data_split)
```

> Model Performance

```{r Comparing models}
# PERFORMANCE ----

## Random Forest ----

### Price with BoxCox ----
performance_rf_price_box <- 
  fit_rf_price_box %>% 
  collect_metrics() %>% 
  mutate(algorithm = "Random Forest for price with BoxCox")

### Price ----
performance_rf_price <- 
  fit_rf_price %>% 
  collect_metrics() %>% 
  mutate(algorithm = "Random Forest for price")

## XG Boost ----

### Price with BoxCox ----
performance_xg_price_box <- 
  fit_xg_price_box %>% # for_performance(fit_xg) %>% 
  collect_metrics() %>%
  mutate(algorithm = "XG Boost for price with BoxCox")

### Price ----
performance_xg_price <- 
  fit_xg_price %>% # for_performance(fit_xg) %>% 
  collect_metrics() %>%
  mutate(algorithm = "XG Boost for price")

## Linear regression ----

### Price with BoxCox ----
performance_ols_price_box <- 
  fit_ols_price_box %>% 
  collect_metrics() %>% 
  mutate(algorithm = "Linear regression for price with BoxCox")

### Price ----
performance_ols_price <- 
  fit_ols_price %>% 
  collect_metrics() %>% 
  mutate(algorithm = "Linear regression for price")

# COMPARE PERFORMANCE OF DIFFERENT ALGORITHMS AND RECIPES ----

bind_rows(performance_rf_price_box,
          performance_rf_price,
          performance_xg_price_box,
          performance_xg_price,
          performance_ols_price_box, 
          performance_ols_price) %>%
  select(-.estimator,
         -.config) %>% 
  pivot_wider(names_from = .metric,
              values_from = .estimate) %>% 
  datatable() %>% 
  formatRound(columns = c("rmse",
                          "rsq"),
              digits = 2)
```

> Collect predictions

```{r Predictions}
# CHECK PREDICTIONS ----
## Random Forest ----
### Price with BoxCox ----
prediction_rf_price_box <- 
  fit_rf_price_box %>% 
  collect_predictions() %>%
  mutate(algorithm = "Random Forest for price with BoxCox")

### Price ----
prediction_rf_price <- 
  fit_rf_price %>% 
  collect_predictions() %>%
  mutate(algorithm = "Random Forest for price")

## XG Boost ----
### Price with BoxCox ----
prediction_xg_price_box <-
  fit_xg_price_box %>%
  collect_predictions() %>%
  mutate(algorithm = "XG Boost for price with BoxCox") 

### Price ----
prediction_xg_price <-
  fit_xg_price %>%
  collect_predictions() %>%
  mutate(algorithm = "XG Boost for price")

## Linear regression ----
### Price with BoxCox ----
prediction_ols_price_box <- 
  fit_ols_price_box %>% 
  collect_predictions() %>%
  mutate(algorithm = "Linear regression for price with BoxCox")

### Price ----
prediction_ols_price <- 
  fit_ols_price %>% 
  collect_predictions() %>%
  mutate(algorithm = "Linear regression for price")

# CONSOLIDATE PREDICTIONS ----

cleaned_data <- 
  cleaned_data %>% 
  tibble::rowid_to_column(".row")

data_test_with_row <- 
  data_test %>%
  rename(.row = id)
```

> Model Plots

```{r Model Plots}
# CREATE PLOTS ----
## Random Forest ----

### Price with BoxCox ----
rf_price_box_plot <- 
  prediction_rf_price_box %>% 
  select(.row, log10_price, .pred)%>%
  ggplot(aes(x = log10_price,
             y = .pred)
  ) + 
  geom_point(color = "dodgerblue",
             alpha = 0.50) + 
  geom_abline(color = "red",
              lty = 2) +
  labs(y = "Predicted Price of Airbnb",
       x = "Actual price of Airbnb",
       title = "Predicting Airbnb price in NYC using RF with BoxCox") + 
  theme_bw()

### Price ----
rf_price_plot <- 
  prediction_rf_price %>% 
  select(.row, log10_price, .pred) %>%
  ggplot(aes(x = log10_price,
             y = .pred)
  ) + 
  geom_point(color = "dodgerblue",
             alpha = 0.50) + 
  geom_abline(color = "red",
              lty = 2) +
  labs(y = "Predicted Price of Airbnb",
       x = "Actual price of Airbnb",
       title = "Predicting Airbnb price in NYC using RF") + 
  theme_bw()

## XG Boost ----

###Price with BoxCox ----

xg_price_box_plot <- 
  prediction_xg_price_box %>% 
  select(.row, log10_price, .pred) %>%
  ggplot(aes(x = log10_price,
             y = .pred)
  ) + 
  geom_point(color = "dodgerblue",
             alpha = 0.50) + 
  geom_abline(color = "red",
              lty = 2) +
  labs(y = "Predicted Price of Airbnb",
       x = "Actual price of Airbnb",
       title = "Predicting Airbnb price in NYC using XG with BoxCox") + 
  theme_bw()

### Price ----
xg_price_plot <- 
  prediction_rf_price %>% 
  select(.row, log10_price, .pred) %>%
  ggplot(aes(x = log10_price,
             y = .pred)
  ) + 
  geom_point(color = "dodgerblue",
             alpha = 0.50) + 
  geom_abline(color = "red",
              lty = 2) +
  labs(y = "Predicted Price of Airbnb",
       x = "Actual price of Airbnb",
       title = "Predicting Airbnb price in NYC using XG") + 
  theme_bw()

## OLS ----

### Price with BoxCox ----

ols_price_box_plot <- 
  prediction_ols_price_box %>% 
  select(.row, log10_price, .pred) %>%
  ggplot(aes(x = log10_price,
             y = .pred)
  ) + 
  geom_point(color = "dodgerblue",
             alpha = 0.50) + 
  geom_abline(color = "red",
              lty = 2) +
  labs(y = "Predicted Price of Airbnb",
       x = "Actual price of Airbnb",
       title = "Predicting Airbnb price in NYC using OLS with BoxCox") + 
  theme_bw()

ols_price_plot <- 
  prediction_ols_price %>% 
  select(.row, log10_price, .pred) %>%
  ggplot(aes(x = log10_price,
             y = .pred)
  ) + 
  geom_point(color = "dodgerblue",
             alpha = 0.50) + 
  geom_abline(color = "red",
              lty = 2) +
  labs(y = "Predicted Price of Airbnb",
       x = "Actual price of Airbnb",
       title = "Predicting Airbnb price in NYC using OLS") + 
  theme_bw()

grid.arrange(rf_price_box_plot,
             rf_price_plot,
             xg_price_box_plot,
             xg_price_plot,
             ols_price_box_plot,
             ols_price_plot,
             ncol = 2)
```

> Feature Importance

```{r Feature Importance}
# FINALIZED MODEL ----
## Feature Importance ----

library(vip)

# VERY IMPORTANT ----
finalized_model <- 
  finalized_workflow_xg_price %>% 
  fit(cleaned_data)

feature_importance_PREP <-
  XG_BOOST %>% # model
  finalize_model(select_best(tuned_xg_price)
  ) %>%
  set_engine("xgboost",
             importance = "permutation")

feature_importance <- 
  workflow() %>% 
  add_recipe(recipe_price) %>% 
  add_model(feature_importance_PREP) %>% 
  fit(data_train) %>% 
  extract_fit_parsnip() %>% # pull_workflow_fit()
  vip(aesthetic = list(fill = "deepskyblue2",
                       alpha = 0.50)
  ) 

feature_importance + 
  theme_bw()
```

> Deploy Machine Learning Algorithm to Dashboard

```{r Deploy}
finalized_model %>% saveRDS("finalized_model.rds")
```

> Save R Data 

```{r RData}
save.image("project_airbnb.RData")
```

## 5. Executive Summary

### 5.1. Evidence

* Based on initial examination, we removed 4 variables as they did not have an impact on the prediction model, namely, **name**, **host_name**, **host_id**, **last_review**.

* Based on the observations of feature engineering, two recipes were created. In both recipes, the log10_price variable was used as the output variable. Since the dataset contained a lot of neighbourhoods, all neighbourhoods that represented less than 5% of total data were classified as 'Other' neighbourhoods.

* In recipe 1, log10_price was expressed as a function of all other variables remaining after cleaning the data. The variable 'calculated_host_listings_count' was subjected to a BoxCox transformation using the step_BoxCox function.

* In recipe 2, log10_price was expressed as a function of certain variables chosen depending on their importance observed through the correlation matrix. This recipe did not contain the variable that was subjected to BoxCox transformation.

* The above 2 recipes, combined with the 3 different models, gave us 6 model-recipe algorithm combinations.

* Upon comparing the different models, we observed that `XG Boost Algorithm` without the recipe containing the BoxCox transformation exhibited the least RMS Error at **0.18** and R² value at **0.62**.

*  When we observe the feature importance for this model, we see the following result: **Room type** is the most important feature when it comes to determining the price of an Airbnb listing. 

### 5.2. Interpretation

* Based on the above plots and the table portraying the performance metrics of different model-recipe combinations, we have chosen the XG Boost model with the recipe without BoxCox transformation as our final model. 

* `XG Boost Algorithm` performed better overall with a rmse of **0.18** and R squared of **0.62**.

* **Room types** and **neighborhood_group** play important roles in determining the price per night of Airbnb in NYC.

### 5.3. Recommendations

* The key use application for this predictive model is in determining the room price of an airbnb based on the factors such as area, room type, reviews, and minimum nights. 
Hence, the key stakeholders whom would most benefit from this predictive model are `(1) Airbnb hosts`, `(2) Airbnb Company` and `(3) the public`. 

* This algorithm takes into account several variables to produce a evidence-based prediction of the prices of Airbnb in NYC. For `Airbnb hosts`, this predictive model is a useful data tool that can be used in conjunction with qualitative checks conducted based on their expertise. This would help them make **more informative decision regarding the price**.

* One of the drivers for this predictive model study was the [average price of Airbnb by city](https://www.alltherooms.com/analytics/average-airbnb-prices-by-city/) leading to competitive price, which could have been improved through identifying the drivers so that hosts in NYC can efficiently price.

* Hence, it is recommended that this prediction model be **made openly available to potential Airbnb hosts to input their available factors for guidance** to arrive at a predicted price range of Airbnb in NYC. The current Airbnb hosts who want to attract more renters could also use this model to understand their positions so that they can make informed decisions.

## Limitations

* This study applies to Airbnb prices in NYC only. 

* The results derived from this predictive model only indicates specific neighbor groups. It does not provide information about areas outside available neighbor groups in the data set. 

* This model is easy and accessible to use by giving general idea of prices set by the owners. For outlier room types, this model would not fit.  

* However, the data set lacks other potential outside factors such as travel disruptions caused by COVID 19. Hence, further data collection and analysis if needed must be pursued before this model can be applied to test appropriate price range. 

* **Disclaimer**: Building the model with the original R script showed that XG Boost without BoxCox had the best performance with the lowest rmse and highest R squared values. However, the R markdown file seemed to sometimes render different results and show other models having better performance. Given the inconsistency in performance values, the team decided to stick with the original model that was used to generate the submitted RDS file and build the ShinyApp.

## References

* [Kaggle Data](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)

* [Average hotel prices per night in NYC](https://www.kayak.com/New-York-Hotels.15830.hotel.ksp)

* [The average monthly room price in NYC](https://smartasset.com/mortgage/what-is-the-cost-of-living-in-new-york-city)

* [The most expensive room price compared by the national average](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7399214/)

* [average price of Airbnb by city](https://www.alltherooms.com/analytics/average-airbnb-prices-by-city/)

## Appendix

* Nil.

## Contribution Statement

> The `pRoject` team worked together to deliver the assignment. Together we defined the problem, identifed the data source and segregated tasks to deliver the final output.

  * **R-script** was led by `Sanath Manjunath Nadig`

  * **Shiny-dashboard** was led by `Janelle Ashley SY`

  * **R-markdown** was led by `Kushal Agarwal, Suel Ki Park`

> `Sanath Manjunath Nadig's` contributions:

1)	Collaborated with team to develop the R-script – Transform & EDA, Split, Pre-process, Fit, Tune and Assess.

2) Collaborated to test and find the best recipe and model with rmse.

3) Conducted research to discover interaction between variables.
  
4) Analyzed data to identify top variables that impact market price of Airbnb in NYC.

5) Created visualizations and graphs for analyzing the data and variables.

>  `Janelle Ashley SY` contributions:

1) Collaborated with team to develop the R-script – Transform & EDA, Split, Pre-process, Fit, Tune and Assess.

2) Partnered with team for successful deployment of R-dashboard and debugged error messages preventing successful deployment 

3) Updated the dashboard according to our features to create the Airbnb Prediction App.

4) Partnered with team to draft the Rmarkdown file.
 
5) Finalized the models, tested model on Dashboard, added information to Rmarkdown file. 
  
>  `Kushal Agarwal's` contributions:

1) Collaborated with team to develop the R-script – Transform & EDA, Split, Pre-process, Fit, Tune and Assess.

2) Collaborated with teammate to formulate the business problem.
  
3) Partnered with team to draft the Rmarkdown file.

4) Helped in writing the Executive Summary – Evidence, Interpretation and Recommendations.

>  `Suel Ki Park's` contributions:

1) Collaborated with team to develop the R-script – Transform & EDA, Split, Pre-process, Fit, Tune and Assess.

2) Collaborated with teammate to formulate the business problem.
 
3) Partnered with team to draft the Rmarkdown file.

4) Helped in writing the Executive Summary – Evidence, Interpretation and Recommendations.

<br>
